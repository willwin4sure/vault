Reward only tells you whether your current answer is right or wrong. There is no gradient that pushes you towards the *correct answer*, as in supervised learning. 

Therefore, we need a good amount of ==exploration== to determine what the best actions are. On the other hand, if we already have a relatively good strategy, we want to use ==exploitation== to farm the most value out of it.