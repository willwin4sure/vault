#MIT #CS #performance #software #multicore #algos

Parallel programs run their code simultaneously on multiple processing cores.

Note the distinction between parallelism and concurrency, which is just the notion of having multiple threads of execution (these potentially *could* be run in parallel, or just context switched between on a single core).

This is another critical speedup in performance-intensive applications, and can give a large constant factor on multicore machines (~4x, or even up to ~18x on the right hardware).

## Main Sequence

From the first part of the course:

1. [[Multicore Programming]]
2. [[Races and Parallelism]]
3. [[DAGs, Work, and Span]]
4. [[Scheduling Theory and Parallel Loops]]
5. [[Task-Parallel Algorithms]]

And we return after [[⛺Memory and Caching Homepage]].

6. [[Nondeterministic Parallel Programming]]
7. [[Synchronization Without Locks]]
8. [[Speculative Parallelism]]

---

**Next:** [[⛺Memory and Caching Homepage]]