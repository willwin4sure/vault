#MIT #CS #performance #software #multicore #algos

Parallel programs run their code simultaneously on multiple processing cores.

Note the distinction between parallelism and [[concurrency]], which is just the notion of having multiple threads of execution (these potentially *could* be run in parallel, or just context switched between on a single core).

This is another critical speedup in performance-intensive applications, and can give a large constant factor on multicore machines (~4x, or even up to ~18x on the right hardware).

## Main Sequence

From the first part of the course:

1. [[Multicore Programming]]
2. [[Races and Parallelism]]
3. [[DAGs, Work, and Span]]
4. [[Scheduling Theory and Parallel Loops]]
5. [[Task-Parallel Algorithms]]

And we return after [[⛺Memory and Caching Homepage]].

6. [[Nondeterministic Parallel Programming]]
7. [[Synchronization Without Locks]]
8. [[Speculative Parallelism]]

---

**Next:** [[⛺Memory and Caching Homepage]]