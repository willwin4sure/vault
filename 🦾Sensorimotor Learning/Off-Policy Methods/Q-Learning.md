We just saw [[Value Iteration#Q-Value Iteration|Q-value iteration]], and noted that it is off-policy, meaning that $Q$ has no dependence on the policy.

Therefore, we can rewrite the process as the repeated global update
$$
Q^{t}(s,a)=R(s,a)+\gamma \sum_{s' \in S}^{}p(s'|s,a)\max_{a'}Q^{t-1}(s',a').
$$
Once we learn the optimal $Q$-function (convergence is determined when all updates are at most $\varepsilon$), we can extract the optimal policy directly via
$$
\pi^{*}(s)=\arg\max_{a}Q^{*}(s,a).
$$
## Transition Function Dependence

It seems like we still need to know how the state evolves, i.e. $p(s'|s,a)$. Another way to rewrite the $Q$-function is by
$$
Q^{t}(s,a)=\sum_{t=0}^{T}\gamma^{t}r_{t,}
$$
where the rewards $r_{t}$ are sampled from trajectories generated by the policy $\pi_{t-1}$ that just picks the argmax of $Q^{t-1}$ over actions at any state.

Of course, this way of writing the $Q$-function has higher variance, since if we perform Monte-Carlo rollouts it is less accurate than actually computing via the transition probabilities.

## Gradual Updates

So if we don't know the actual transition model (or the state/action space is continuous), we need to estimate it by collecting data from rollouts. Suppose we have access to the environment and can collect transition data $(s,a,r,s')$. 

For each data point, we will gradually nudge our $Q$-function in the target direction. This also keeps us robust against bad trajectories.

> [!example] (Q-learning)
> Compute a target for $Q^{t}(s,a)$ via 
> $$y_{t}=r+\max_{a'}Q^{t-1}(s',a'),$$
> and then update $Q^{t}(s,a)$ in the direction of $y^{t}$ with $\alpha \in(0,1)$ times the error. We usually want this learning rate $\alpha$ to be small. In particular,
> $$
> Q^{t}(s,a)=Q^{t-1}(s,a)+\alpha \left( r+\max_{a'}Q^{t-1}(s',a')-Q^{t-1}(s,a) \right).
> $$

One important choice in the algorithm is how we initialize $Q$-values. This will tend to affect which states you end up exploring. One nice option to increase exploration is to be overly optimistic about actions, setting all $Q$-values to be relatively high. 

The policy that we get is $\pi(s)=\arg\max_{a}Q^{t-1}(s,a)$, which we will use for the next data collection step.

Of course, we can also apply a UCB-style technique from [[Upper Confidence Bound Algorithm|our study of bandits]], adding an uncertainty term to each action inversely related to the number of times that action has been sampled. This looks a lot like what happens in the [[The UCT Algorithm|UCT algorithm]] for our final project SPRL.

Counts only work in discrete spaces. How can we encourage exploration if the state/action spaces are continuous?

> [!example] ($\varepsilon$-greedy)
> You've seen this before: with probability $1-\varepsilon$, just greedy and pick the argmax. With probability $\varepsilon$, sample randomly.

## Off-Policy Learning

Earlier, we gave an advertisement that $Q$-learning is off-policy. What is the advantage of this compared to [[â›ºOn-Policy Learning Homepage|on-policy methods]]?

Well, suppose we are just handed a dataset of (state, action, reward) tuples from some unknown source. We *cannot* run the on-policy PPO algorithm at all! In particular, policy gradients require that the data contains trajectories that come from the policy you are trying to improve. 

What about for Q-learning? It seems like the algorithm described above does not incorporate arbitrary data; instead, it samples it from the current best policy using access to the environment. 

Well, you actually just can, just replace the repeated trajectory sampling procedure with the data that already exists (randomly sample a trajectory from the data, say).

> [!idea]
> Q-learning has the benefit over PPO of being off-policy, but it is often more unstable when implemented.

---

**Next:** [[Deep Q-learning]]








