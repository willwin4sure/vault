A random variable has an interpretation as a certain quantity, or state, determined by chance. For example, the number of heads that occur after flipping 100 coins. This *compresses* the information of the exact outcome that occurred (the exact sequence of coin flips) into a single real number.

## Definitions

> [!definition] Definition (Random variable)
> Let $(\Omega,\mathcal{F},\mathbb{P})$ be a probability space and let $(E,\mathcal{E})$ be a measurable space. A measurable function $X:\Omega\to E$ is called a ==random variable== in $E$. If $E$ is not specified, we assume that $X$ takes values in $\mathbb{R}$.

^e3ffe4

If you've seen $\mathbb{R}$-valued random variables before, it is common to talk about their cumulative distribution function:

> [!definition] Definition (Distributions)
> The image measure $\mu_{X}=\mathbb{P}\circ X ^{-1}$ is called the ==law==, or ==distribution==, of $X$. For $\mathbb{R}$-valued random variables, $\mu_{X}$ is uniquely determined by its values on the $\pi$-system of intervals $((-\infty,x]: x \in \mathbb{R})$, given by
> $$
> F_{X}(x)=\mu_{X}((-\infty,x])=\mathbb{P}(X\leq x).
> $$
> Then, the function $F_{X}$ is called the ==distribution function== of $X$. Note that $F_{X}$ is non-decreasing and right-continuous, with
> $$
> \lim_{ x \to -\infty } F_{X}(x) = 0, \quad \lim_{ x \to \infty } F_{X}(x)=1.
> $$
> Any function satisfying such properties is called, generally, a distribution function.

It turns out that given any distribution function (i.e. a CDF), we can *construct* a random variable with that distribution function.

> [!proposition] Proposition (Building a random variable from a CDF)
> Every distribution function $F$ is the distribution function of some random variable.

^d66555

> [!proof]-
> Let $\Omega=(0,1)$ and take $(\Omega,\mathcal{F},\mathbb{P})$ as the probability space where $\mathcal{F}$ is the Borel $\sigma$-algebra on $\Omega$ and $\mathbb{P}$ is the restriction of the Lebesgue measure on $\mathcal{F}$. Then, take $X:\Omega\to \mathbb{R}$ by
> $$
> X(\omega)=\inf\{ x:\omega \leq F(x) \}.
> $$
> In other words, in order to achieve a certain distribution $F$, we take the uniform distribution on $(0,1)$ and push it through the inverse CDF. By [[Image Measures#^6db858|a lemma from last section]], $X(\omega)\leq x$ if and only if $\omega \leq F(x)$. Therefore,
> $$
> F_{X}(x)=\mathbb{P}(X\leq x) = \mathbb{P}((0,F(x))) = F(x),
> $$
> as desired.

## Independence

We've already seen what it means for a family of events or $\sigma$-algebras to be [[Independence|independent]]. Now, we will define what it means for a set of random variables to be independent. Well, random variables are just measurable functions, and we know how to [[Measurable Functions#^7434a6|generate $\sigma$-algebras from them]]. All we need is for those $\sigma$-algebras to be independent.

> [!definition] Definition (Independence)
> We say that a countable family of random variables $(X_{i}: i \in I)$ is ==independent== if the family of $\sigma$-algebras $(\sigma(X_{i}):i\in I)$ is independent. For a sequence $(X_{n}:n\in \mathbb{N})$ of $\mathbb{R}$-valued random variables, this is equivalent to the condition
> $$
> \mathbb{P}(X_{1}\leq x_{1},\dots,X_{n}\leq x_{n})=\mathbb{P}(X_{1}\leq x_{1})\cdots \mathbb{P}(X_{n}\leq x_{n}).
> $$
> for all $x_{i}\in \mathbb{R}$ and all $n$, as we [[Independence#^c79539|only need to check on a $\pi$-system]].

^1182a1

It turns out that we can use a trick to construct any countable sequence of independent random variables as well. Read here:

1. [[Rademacher Functions]]

## Processes

> [!definition]
> A sequence of random variables $(X_{n}:n\geq 0)$ is often regarded as a ==process== evolving over time as $n$ increases. The $\sigma$-algebra generated by $X_{0},\dots,X_{n}$
> $$
> \mathcal{F}_{n}=\sigma(X_{0},\dots,X_{n})
> $$
> contains those events depending (measurably) on $X_{0},\dots,X_{n}$ and represents what could be known about the process by time $n$.

> [!example]
> For example, if $X_{t}$ is a Markov chain, then $\sigma(X_{t})$ corresponds to events of the form "the $t$-th state of the Markov chain is in some subset". Then, $\mathcal{F}_{n}$ consists of sets of sequences, telling us what may have happened in the first $n$ transitions.

---

**Next:** [[Convergence of Measurable Functions]]