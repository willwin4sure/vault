> Alternate between updating your best guess for hidden data and parameters.

This is one method of implementing [[Maximum Likelihood Estimation|maximum likelihood estimation]]. 
## Motivation

Suppose we have a mixture of two Gaussians $\mathtt{N}(\mu_{0},\sigma_{0}^{2})$ and $\mathtt{N}(\mu_{1},\sigma_{1}^{2})$. Every observation $y_{n}$ that we see is internally labeled with an extra label $w_{n}\in \{ 0,1 \}$ sampled from a Bernoulli random variable which indicates which distribution it was generated from, but this is unknown to us. 

If an oracle gave us access to $w_{n}$, then ML estimation would be trivial: simply sort the observations by $i$ and then separately solve the optimizations
$$
(\hat{\mu}_{i},\hat{\sigma}_{i}^{2})=\mathop{\arg\max}_{(\mu,\sigma^{2})}\sum_{n=1}^{N}\mathbf{1}_{w_{n}=i}\left(-\ln\sigma-\frac{1}{2}\left( \frac{y_{i}-\mu}{\sigma} \right)^{2}\right)
$$
for each $i$. Yet we don't know the labels, so this approach fails. Directly solving for the maximum likelihood across all the labels and parameters is also infeasible.

> [!idea]
> Make guesses for the labels and parameters, and iteratively update each.

In the loss landscape, we make axis-aligned steps corresponding to these two types of parameters.

> [!example] (Iterative Parameter Estimation)
> 1. First guess arbitrarily the parameters of the Gaussians and the mixture coefficient $q=p_{\mathsf{w}}(1)$.
> 2. Then, use these guesses to estimate the probability for each data point that it came from each mixture component, i.e. compute $p_{\mathsf{w}|\mathsf{y}}(\bullet|y_{n})$ for each $n$.
> 3. Update the guesses for the parameters by solving the optimization problem above, with $\mathbf{1}_{w_{n}=i}$ replaced with $p_{\mathsf{w}|\mathsf{y}}(i|y_{n})$ instead.
> 4. Update your guess $\hat{q}$ by averaging the probabilities $p_{\mathsf{w}|\mathsf{y}}(1|y_{n})$ you computed before.

## Complete Data and the Pseudo Log-Likelihood

Now let's formalize the problem and algorithm a bit more. Suppose we have some *observed data* $\mathbf{y}$ that is a realization of the random variable $\boldsymbol{\mathsf{y}}$ with distribution $p_{\boldsymbol{\mathsf{y}}}(\bullet;\mathbf{x})$ for some $\mathbf{x} \in \mathcal{X}$. Our goal is to compute the ML estimate
$$
\hat{\mathbf{x}}=\mathop{\arg\max}_{\mathbf{a}\in \mathcal{X}}\ \ell_{\boldsymbol{\mathsf{y}}}(\mathbf{a};\mathbf{y}),\quad \ell_{\boldsymbol{\mathsf{y}}}(\mathbf{a};\mathbf{y})=\log p_{\boldsymbol{\mathsf{y}}}(\mathbf{y};\mathbf{a}).
$$

> [!definition] (Complete data)
> We define a second variable $\boldsymbol{\mathsf{z}}$ called the ==complete data==, that is generated by a probability distribution $p_{\boldsymbol{\mathsf{z}}}(\bullet;\mathbf{x})$ but is not fully observable. The only part we can see is $\boldsymbol{\mathsf{y}}=\mathbf{g}(\boldsymbol{\mathsf{z}})$, where $\mathbf{g}$ is a deterministic function. 

We want to design $\boldsymbol{\mathsf{z}}$ so that solving the optimization problem
$$
\hat{\mathbf{x}}=\mathop{\arg\max}_{\mathbf{a}\in \mathcal{X}}\ \ell_{\boldsymbol{\mathsf{z}}}(\mathbf{a};\mathbf{z}),\quad \ell_{\boldsymbol{\mathsf{z}}}(\mathbf{a};\mathbf{z})=\log p_{\boldsymbol{\mathsf{z}}}(\mathbf{z};\mathbf{a})
$$
is computationally tractable, or at least easier than the previous problem (e.g. imagine having the labels in addition to the data in the mixture of Gaussians problem).

We cannot evaluate $\ell_{\boldsymbol{\mathsf{z}}}(\bullet;\mathbf{z})$ directly since the complete data is not observable, but we can evaluate its expectation given the observation $\mathbf{y}$.

> [!definition] (Pseudo log-likelihood)
> We define the ==pseudo log-likelihood== via
> $$
> 	\hat{\ell}_{\boldsymbol{\mathsf{z}}}'(\mathbf{x};\mathbf{y})=\mathbb{E}_{p_{\boldsymbol{\mathsf{z}|\boldsymbol{\mathsf{y}}}}(\bullet|\mathbf{y};\mathbf{x}')}\left[ \ell_{\boldsymbol{\mathsf{z}}}(\mathbf{x};\boldsymbol{\mathsf{z}}) \right]
> $$
> for an arbitrary $\mathbf{x}'\in \mathcal{X}$, which represents your current guess about $\mathbf{x}$. We also denote this $U(\mathbf{x};\mathbf{x}')$.

Now, our goal was to maximize
$$
\log p_{\boldsymbol{\mathsf{y}}}(\mathbf{y};\mathbf{x})=\log p_{\boldsymbol{\mathsf{z}}}(\mathbf{z};\mathbf{x})-\log p_{\boldsymbol{\mathsf{z}}|\boldsymbol{\mathsf{y}}}(\mathbf{z}|\mathbf{y};\mathbf{x}).
$$
Take the expectation of both sides with respect to $p_{\boldsymbol{\mathsf{z}}|\boldsymbol{\mathsf{y}}}(\bullet|\mathbf{y};\mathbf{x}')$ for our current guess $\mathbf{x}'$. 
$$
\log p_{\boldsymbol{\mathsf{y}}}(\mathbf{y};\mathbf{x})=\underbrace{\mathbb{E}_{p_{\boldsymbol{\mathsf{z}}|\boldsymbol{\mathsf{y}}}(\bullet|\mathbf{y};\mathbf{x}')}\left[ \log p_{\boldsymbol{\mathsf{z}}}(\mathbf{z};\mathbf{x}) \right]}_{U(\mathbf{x};\mathbf{x}')} \underbrace{- \mathbb{E}_{p_{\boldsymbol{\mathsf{z}}|\boldsymbol{\mathsf{y}}}(\bullet|\mathbf{y};\mathbf{x'})}\left[ \log p_{\boldsymbol{\mathsf{z}}|\boldsymbol{\mathsf{y}}}(\mathbf{z}|\mathbf{y}; \mathbf{x}) \right]}_{V(\mathbf{x};\mathbf{x}')}.
$$
Basically, the current guess $\mathbf{x}'$ *only* gets used to get a distribution over $\mathbf{z}$ values; we want to maximize the likelihood that these values occur with our new $\mathbf{x}$.

Now, we want to pick some $\mathbf{x}$ that maximizes both $U$ and $V$. However, it turns out that $V$ takes care of itself by [[Some Useful Inequalities#^e52776|Gibbs' inequality]], which states that
$$
V(\mathbf{x};\mathbf{x}')\geq -\mathbb{E}_{p_{\boldsymbol{\mathsf{z}}|\boldsymbol{\mathsf{y}}}(\bullet|\mathbf{y};\mathbf{x}')}\left[ \log p_{\boldsymbol{\mathsf{z}}|\boldsymbol{\mathsf{y}}}(\mathbf{z}|\mathbf{y};\mathbf{x}') \right]=V(\mathbf{x'};\mathbf{x}').
$$
Therefore, it is enough to maximize the pseudo log-likelihood $U(\mathbf{x};\mathbf{x}')$ and the log-likelihood will grow, so long as $\mathbf{x}\neq \mathbf{x}'$.

## Full Algorithm

> [!example] (EM Algorithm)
> 1. Set $l=0$ and guess a parameter estimate $\hat{\mathbf{x}}^{(0)}$.
> 2. **E-step:** Compute the expectation
> $$
> U(\mathbf{x};\hat{\mathbf{x}}^{(l)})=\mathbb{E}_{p_{\boldsymbol{\mathsf{z}}|\boldsymbol{\mathsf{y}}}(\bullet|\mathbf{y};\hat{\mathbf{x}}^{(l)})}\left[ \log p_{\boldsymbol{\mathsf{z}}}(\boldsymbol{\mathsf{z}};\mathbf{x}) \right].
> $$
> 3. **M-step:** Solve the maximization
> $$
> \hat{\mathbf{x}}^{(l+1)}=\mathop{\arg\max}_{\mathbf{x}\in \mathcal{X}}\ U(\mathbf{x};\hat{\mathbf{x}}^{(l)}).
> $$
> 4. Increment $l$ and go to step 2, repeating until convergence.

From our work before, the sequence of log-likelihoods $\ell_{\boldsymbol{\mathsf{y}}}(\hat{\mathbf{x}}^{(l)};\mathbf{y})$ is non-decreasing in $l$. Optimizing $U$ is maximizing a particular *lower-bound* on the gain on the $l$-th iteration, since we are ignoring the $V$ term.

> [!claim] (Nice properties)
> The EM algorithm is guaranteed to converge to a stationary point of $\ell_{\boldsymbol{\mathsf{y}}}(\bullet;\mathbf{y})$, and when there is a unique such stationary point, the ML estimate is obtained.

The convergence rate is *exponential* in $l$ as $l\to \infty$, but the choice of complete data can have a large impact.

---

**Next:** [[The EM Algorithm on Exponential Families]]
