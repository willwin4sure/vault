#MIT #CS #stats #inference 

High-dimensional inference is really hard to do exactly. In modern machine learning applications, alphabets may be enormous (e.g. NLP), continuous random variables are high dimensional (e.g. vision), and parameter spaces may be huge (e.g. billion parameter neural networks).

In this section of the course, we explore approximation tools to handle this computational infeasibility. These can be broadly split into three categories: *stochastic* approximation, *asymptotic* approximation, and *variational* approximation.

## Main Sequence

Stochastic approximation techniques.

1. [[Monte Carlo Methods]]
2. [[Markov Chain Monte Carlo (MCMC)]]

Asymptotic approximation framework.

4. [[Typicality]]
5. [[Large Deviations and Cram√©r's Theorem]]
6. [[Method of Types]]
7. [[Sanov's Theorem]]

Asymptotic approximation analysis.

8. [[Asymptotics of Hypothesis Testing]]
9. [[Convergence of Random Sequences]]
10. [[Asymptotics of Non-Bayesian Parameter Estimation]]
11. [[Asymptotics of Bayesian Parameter Estimation]]
12. [[Universal Inference]]
13. [[Variational Inference]]