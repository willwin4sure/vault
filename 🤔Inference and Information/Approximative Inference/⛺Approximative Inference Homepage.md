#MIT #CS #stats #inference 

High-dimensional inference is really hard to do correctly. In modern machine learning applications, alphabets may be enormous (e.g. NLP), continuous random variables are high dimensional (e.g. vision), and parameter spaces may be huge (e.g. billion or trillion parameter neural networks).

In this section of the course, we explore approximation tools to handle this computational infeasibility. These can be broadly split into three categories: *stochastic* approximation, *asymptotic* approximation, and *variational* approximation.