#MIT #CS #stats #inference 

Now, we turn to the study of information theory. 

## Main Sequence

First, a discussion of measures of information: entropy, conditional entropy, mutual information, and KL divergence. 

1. [[Generalized Bayesian Decision Theory]]
2. [[Information Measures]]
3. [[The Data Processing Inequality]]
4. [[KL Divergence]]
5. [[Other Divergence Families]]

Next, a discussion of information *geometry*, where we investigate the pseudo-geometry induced by the KL divergence (somewhat analogous to squared Euclidean distance) on the space of distributions.

6. [[The Probability Simplex]]
7. [[Information Projection and Pythagoras' Theorem]]
8. [[Linear Families]]
9. [[Orthogonal Families]]

Finally, an application to the problem of *modeling*, where the goal is to have a good guess for the underlying distribution, rather than the parameter at hand. Our technique will be modeling things as a mixture of the candidate models.

10. [[Modeling as Inference]]
11. [[Mixture Models]]

A quick note about extending to the continuous domain.

12. [[Continuous Information Theory]]
13. [[Information Measures for Gaussians]]

Then, we return to a discussion of modeling. We talk more about priors and how the update them. The familiar concept of *conjugate priors* appears, allowing us to efficiently update our beliefs. Such Bayesian updates also have the nice property of *permutation invariance*.

14. [[Maximally Ignorant Priors]]
15. [[Conjugate Priors]]
16. [[Conjugate Priors and Sufficient Statistics]]
17. [[Conjugate Prior Families for Exponential Families]]

Finally, we return to the technique of the [[The Expectation Maximization (EM) Algorithm|EM algorithm]] and provide an information-geometric viewpoint of it.

18. [[Alternating Projections]]

---
